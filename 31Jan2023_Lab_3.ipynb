{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning\n",
    "    Session 14\n",
    "    OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 13:02:20.053117: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-31 13:02:20.253002: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-31 13:02:21.493719: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/hpcap/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-01-31 13:02:21.493782: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/hpcap/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-01-31 13:02:21.493789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'segmentation_map_to_image' from 'notebook_utils' (/home/hpcap/.local/lib/python3.10/site-packages/notebook_utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Markdown\n\u001b[1;32m     19\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./utils\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnotebook_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m segmentation_map_to_image\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'segmentation_map_to_image' from 'notebook_utils' (/home/hpcap/.local/lib/python3.10/site-packages/notebook_utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from openvino.runtime import Core\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from IPython.display import Markdown\n",
    "\n",
    "sys.path.append(\"./utils\")\n",
    "from notebook_utils import segmentation_map_to_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Basic Parameters for housekeeping\n",
    "\n",
    "# set location of input files:\n",
    "inpDir = os.path.join('..', '..', 'input')\n",
    "\n",
    "# set location of inputs for this module\n",
    "moduleDir = 'machine_learning'\n",
    "# set location of output files\n",
    "outDir = os.path.join('..', 'output')\n",
    "\n",
    "# define and set random state \n",
    "RANDOM_STATE = 24\n",
    "np.random.seed(RANDOM_STATE) # Set Random Seed for reproducible  results\n",
    "\n",
    "# parameters for Matplotlib\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (15, 9),\n",
    "          'axes.labelsize': 'large',\n",
    "          'axes.titlesize':'x-large',\n",
    "          'xtick.labelsize':'x-large',\n",
    "          'ytick.labelsize':'large',\n",
    "          'savefig.dpi': 75,\n",
    "          'image.interpolation': 'none',\n",
    "          'savefig.bbox' : 'tight',\n",
    "          'lines.linewidth' : 1,\n",
    "          'legend.numpoints' : 1\n",
    "         }\n",
    "\n",
    "CMAP = plt.cm.brg\n",
    "plt.rcParams.update(params);\n",
    "plt.set_cmap(CMAP);\n",
    "\n",
    "plt.style.use('bmh')\n",
    "\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenVINO is an open-source toolkit for optimizing and deploying deep learning models. It provides boosted deep learning performance for vision, audio, and language models from popular frameworks like TensorFlow, PyTorch, and more. \n",
    "\n",
    "<a href='https://docs.openvino.ai/latest/home.html' /><img src='images/ov_chart.png' width='600px' alt=\"Open VINO Chart\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenVINO enables you to optimize a deep learning model from almost any framework and deploy it with best-in-class performance on a range of Intel processors and other hardware platforms.\n",
    "\n",
    "A typical workflow with OpenVINO is shown below.\n",
    "\n",
    "<img src='images/openVINO_fig2.png' width='600' alt=\"OpenVINO Workflow\"/>\n",
    "\n",
    "OpenVINO Runtime automatically optimizes deep learning pipelines using aggressive graph fusion, memory reuse, load balancing, and inferencing parallelism across CPU, GPU, VPU, and more. You can integrate and offload to accelerators additional operations for pre- and post-processing to reduce end-to-end latency and improve throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boost your model’s speed even further with quantization and other state-of-the-art compression techniques available in OpenVINO’s Post-Training Optimization Tool and Neural Network Compression Framework. These techniques also reduce your model size and memory requirements, allowing it to be deployed on resource-constrained edge hardware.\n",
    "\n",
    "\n",
    "\n",
    "### Local Inferencing & Model Serving\n",
    "\n",
    "You can either link directly with OpenVINO Runtime to run inference locally or use OpenVINO Model Serving to serve model inference from separate server or within Kubernetes environment\n",
    "\n",
    "### Improved Application Portability\n",
    "\n",
    "Write an application once, deploy it anywhere, achieving maximum performance from hardware. Automatic device discovery allows for superior deployment flexibility. OpenVINO Runtime supports Linux, Windows and MacOS and provides Python, C++ and C API. Use your preferred language and OS.\n",
    "\n",
    "### Minimal External Dependencies\n",
    "\n",
    "Designed with minimal external dependencies reduces the application footprint, simplifying installation and dependency management. Popular package managers enable application dependencies to be easily installed and upgraded. Custom compilation for your specific model(s) further reduces final binary size.\n",
    "\n",
    "\n",
    "\n",
    "### Enhanced App Start-Up Time\n",
    "\n",
    "In applications where fast start-up is required, OpenVINO significantly reduces first-inference latency by using the CPU for initial inference and then switching to GPU or VPU once the model has been compiled and loaded to memory. Compiled models are cached to further improving start-up time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "model = ie.read_model(model=\"../model/v3-small_224_1.0_float.xml\")\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "output_layer = compiled_model.output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MobileNet model expects images in RGB format.\n",
    "image = cv2.cvtColor(cv2.imread(filename=os.path.join(inpDir,'data/image/coco.jpg')),\n",
    "                     code=cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resize to MobileNet image shape.\n",
    "input_image = cv2.resize(src=image, dsize=(224, 224))\n",
    "\n",
    "# Reshape to model input shape.\n",
    "input_image = np.expand_dims(input_image, 0)\n",
    "plt.imshow(image);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_infer = compiled_model([input_image])[output_layer]\n",
    "result_index = np.argmax(result_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenetClassesPath = os.path.join(inpDir,'data/datasets/imagenet/imagenet_2012.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the inference result to a class name.\n",
    "imagenet_classes = open(imagenetClassesPath).read().splitlines()\n",
    "\n",
    "# The model description states that for this model, class 0 is a background.\n",
    "# Therefore, a background must be added at the beginning of imagenet_classes.\n",
    "imagenet_classes = ['background'] + imagenet_classes\n",
    "\n",
    "imagenet_classes[result_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Segmentation\n",
    "\n",
    "A very basic introduction to using segmentation models with OpenVINO™.\n",
    "\n",
    "In this tutorial, a pre-trained [road-segmentation-adas-0001](https://docs.openvino.ai/latest/omz_models_model_road_segmentation_adas_0001.html) model from the [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) is used. ADAS stands for Advanced Driver Assistance Services. The model recognizes four classes: background, road, curb and mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "\n",
    "model = ie.read_model(model=\"../model/road-segmentation-adas-0001.xml\")\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "input_layer_ir = compiled_model.input(0)\n",
    "output_layer_ir = compiled_model.output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The segmentation network expects images in BGR format.\n",
    "image = cv2.imread(filename=os.path.join(inpDir,\"data/image/empty_road_mapillary.jpg\"))\n",
    "\n",
    "rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_h, image_w, _ = image.shape\n",
    "\n",
    "# N,C,H,W = batch size, number of channels, height, width.\n",
    "N, C, H, W = input_layer_ir.shape\n",
    "\n",
    "# OpenCV resize expects the destination size as (width, height).\n",
    "resized_image = cv2.resize(image, (W, H))\n",
    "\n",
    "# Reshape to the network input shape.\n",
    "input_image = np.expand_dims(\n",
    "    resized_image.transpose(2, 0, 1), 0\n",
    ")  \n",
    "plt.imshow(rgb_image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference.\n",
    "result = compiled_model([input_image])[output_layer_ir]\n",
    "\n",
    "# Prepare data for visualization.\n",
    "segmentation_mask = np.argmax(result, axis=1)\n",
    "plt.imshow(segmentation_mask.transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormap, each color represents a class.\n",
    "colormap = np.array([[68, 1, 84], [48, 103, 141], [53, 183, 120], [199, 216, 52]])\n",
    "\n",
    "# Define the transparency of the segmentation mask on the photo.\n",
    "alpha = 0.3\n",
    "\n",
    "# Use function from notebook_utils.py to transform mask to an RGB image.\n",
    "mask = segmentation_map_to_image(segmentation_mask, colormap)\n",
    "resized_mask = cv2.resize(mask, (image_w, image_h))\n",
    "\n",
    "# Create an image with mask.\n",
    "image_with_mask = cv2.addWeighted(resized_mask, alpha, rgb_image, 1 - alpha, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define titles with images.\n",
    "data = {\"Base Photo\": rgb_image, \"Segmentation\": mask, \"Masked Photo\": image_with_mask}\n",
    "\n",
    "# Create a subplot to visualize images.\n",
    "fig, axs = plt.subplots(1, len(data.items()), figsize=(15, 10))\n",
    "\n",
    "# Fill the subplot.\n",
    "for ax, (name, image) in zip(axs, data.items()):\n",
    "    ax.axis('off')\n",
    "    ax.set_title(name)\n",
    "    ax.imshow(image)\n",
    "\n",
    "# Display an image.\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paths of the source and converted models.\n",
    "model_dir = Path(\"../model\")\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = Path(\"../model/v3-small_224_1.0_float\")\n",
    "\n",
    "ir_path = Path(\"../model/v3-small_224_1.0_float.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.MobileNetV3Small()\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert a Model to OpenVINO IR Format\n",
    "\n",
    "### Convert a TensorFlow Model to OpenVINO IR Format\n",
    "\n",
    "Use Model Optimizer to convert a TensorFlow model to OpenVINO IR with `FP16` precision. The models are saved to the current directory. Add mean values to the model and scale the output with the standard deviation with `--scale_values`. With these options, it is not necessary to normalize input data before propagating it through the network. The original model expects input images in `RGB` format. The converted model also expects images in `RGB` format. If you want the converted model to work with `BGR` images, use the `--reverse-input-channels` option. For more information about Model Optimizer, including a description of the command-line options, see the [Model Optimizer Developer Guide](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html). For information about the model, including input shape, expected color order and mean values, refer to the [model documentation](https://docs.openvino.ai/latest/omz_models_model_mobilenet_v3_small_1_0_224_tf.html).\n",
    "\n",
    "First construct the command for Model Optimizer, and then execute this command in the notebook by prepending the command with an `!`. There may be some errors or warnings in the output. When model optimization is successful, the last lines of the output will include `[ SUCCESS ] Generated IR version 11 model.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the command for Model Optimizer.\n",
    "mo_command = f\"\"\"mo\n",
    "                 --saved_model_dir \"{model_path}\"\n",
    "                 --input_shape \"[1,224,224,3]\"\n",
    "                 --model_name \"{model_path.name}\"\n",
    "                 --compress_to_fp16\n",
    "                 --output_dir \"{model_path.parent}\"\n",
    "                 \"\"\"\n",
    "mo_command = \" \".join(mo_command.split())\n",
    "print(\"Model Optimizer command to convert TensorFlow to OpenVINO:\")\n",
    "display(Markdown(f\"`{mo_command}`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Model Optimizer if the IR model file does not exist\n",
    "if not ir_path.exists():\n",
    "    print(\"Exporting TensorFlow model to IR... This may take a few minutes.\")\n",
    "    ! $mo_command\n",
    "else:\n",
    "    print(f\"IR model {ir_path} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Inference on the Converted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "model = ie.read_model(ir_path)\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_key = compiled_model.input(0)\n",
    "output_key = compiled_model.output(0)\n",
    "network_input_shape = input_key.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an Image\n",
    "\n",
    "Load an image, resize it, and convert it to the input shape of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MobileNet network expects images in RGB format.\n",
    "image = cv2.cvtColor(cv2.imread(filename=os.path.join(inpDir, 'data/image/coco.jpg')), code=cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resize the image to the network input shape.\n",
    "resized_image = cv2.resize(src=image, dsize=(224, 224))\n",
    "\n",
    "# Transpose the image to the network input shape.\n",
    "input_image = np.expand_dims(resized_image, 0)\n",
    "\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compiled_model(input_image)[output_key]\n",
    "\n",
    "result_index = np.argmax(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the inference result to a class name.\n",
    "imagenet_classes = open(imagenetClassesPath).read().splitlines()\n",
    "\n",
    "# The model description states that for this model, class 0 is a background.\n",
    "# Therefore, a background must be added at the beginning of imagenet_classes.\n",
    "imagenet_classes = ['background'] + imagenet_classes\n",
    "\n",
    "imagenet_classes[result_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 1000\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for _ in range(num_images):\n",
    "    compiled_model([input_image])\n",
    "\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "\n",
    "print(\n",
    "    f\"IR model in OpenVINO Runtime/CPU: {time_ir/num_images:.4f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
